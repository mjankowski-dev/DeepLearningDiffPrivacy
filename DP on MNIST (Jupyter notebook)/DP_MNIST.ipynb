{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNFVMtUhMt7l"
      },
      "source": [
        "# Deep learning with Differential Privacy - Main script. Author: Reinier Vos (4663160 TUD) Maciej Jankowski (4651294 TUD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NenrAcsiM7Zl"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkMXve8XuN5X"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Normalization\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from tqdm import tqdm # gives progress bar when loading\n",
        "#import tensorflow_datasets as tfds\n",
        "import matplotlib.ticker as mticker\n",
        "import scipy.stats as sc\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "# verbosity \n",
        "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.autograph.set_verbosity(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NVY1eWCTPIn"
      },
      "source": [
        "# Script Main Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt1MDy1PTPIo"
      },
      "outputs": [],
      "source": [
        "# main\n",
        "seed = 2\n",
        "std_pca = 7 #16 #4 # std for pca\n",
        "std_sgd = 4 # std for dp_sgd\n",
        "batch_size = 600 \n",
        "lr_sgd = 0.05 # [0.01,0.07] stable, best at 0.05\n",
        "C = 4 # gradient clipping bound\n",
        "gs = batch_size\n",
        "\n",
        "# moment accountant specific\n",
        "parameters_ma = {\"maxOrder\":32,\n",
        "                 \"sigma\": std_sgd,\n",
        "                 \"q\": batch_size/60000,\n",
        "                 \"T\":400}\n",
        "debug = True\n",
        "#'''\n",
        "deltaFixed = False\n",
        "epsFixed= True\n",
        "epsilon = 2\n",
        "th_delta = 100#10**-5 # epsilon fixed\n",
        "'''\n",
        "deltaFixed = True \n",
        "epsFixed= False\n",
        "delta = 10e-5\n",
        "th_epsilon = 2 # delta fixed\n",
        "'''\n",
        "allParameters = {**parameters_ma,\n",
        "                'seed' : seed,\n",
        "                'std_pca' :std_pca, \n",
        "                'std_sgd' : std_sgd,\n",
        "                'batch_size' : batch_size, \n",
        "                'lr_sgd' : lr_sgd, \n",
        "                'C' : C, \n",
        "                'deltaFixed' : deltaFixed, \n",
        "                'epsFixed': epsFixed, \n",
        "                'epsilon' : epsilon, \n",
        "                'th_delta' : th_delta, \n",
        "}\n",
        "'''\n",
        "allParameters = {**parameters_ma,\n",
        "                'seed' = seed,\n",
        "                'std_pca' =std_pca, \n",
        "                'std_sgd' = std_sgd,\n",
        "                'batch_size' = batch_size, \n",
        "                'lr_sgd' = lr_sgd, \n",
        "                'C' = C, \n",
        "                'deltaFixed' = deltaFixed, \n",
        "                'epsFixed'= epsFixed, \n",
        "                'delta' = delta, 'th_epsilon' = th_epsilon, \n",
        "}\n",
        "'''\n",
        "batches = 100\n",
        "np.random.seed(seed)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtcG5Of7M-IV"
      },
      "source": [
        "## Load and Preprocess Data\n",
        "We frist load the MNIST dataset using Tensorflow Datasets. This dataset has 28 x 28 grayscale images of digits belonging to 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oz_fEHzTPIp"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELqPqGDvTPIp"
      },
      "outputs": [],
      "source": [
        "# reshape\n",
        "x_train = x_train.reshape(x_train.shape[0],784)\n",
        "x_test = x_test.reshape(x_test.shape[0],784)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "covar = tf.cast(tf.einsum('ji,jk->ik', x_train, x_train), 'float32')\n",
        "shape = tf.shape(covar)\n",
        "noise = tf.random.normal(shape, mean=0, stddev=16, dtype=tf.dtypes.float32)\n",
        "noise = (noise + noise.T)/2\n",
        "covar += noise # covariance matrix used for pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTZgFLP7TPIq"
      },
      "outputs": [],
      "source": [
        "# get batches\n",
        "indices = np.arange(0,len(x_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "x_train_batches = np.array_split(x_train[indices,:], batches)\n",
        "y_train_batches = np.array_split(y_train[indices], batches)\n",
        "train = list(zip(x_train_batches, y_train_batches))\n",
        "x_test_batches = np.array_split(x_test, batches)\n",
        "y_test_batches = np.array_split(y_test, batches)\n",
        "test = list(zip(x_test_batches, y_test_batches))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbliOEMHNiug"
      },
      "outputs": [],
      "source": [
        "class_names = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn718Y0LOIaY"
      },
      "source": [
        "Next, you normalize the images by dividing them by 255.0 so as to make the pixels fall in the range (0, 1). You also reshape the data so as to flatten the 28 x 28 pixel array into a flattened 784 pixel array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws3N-uOgOnMf"
      },
      "source": [
        "Now you shuffle and batch your training and test datasets before feeding them to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuCf0s7eOxKQ"
      },
      "source": [
        "## Define the Model - DP for MNIST as in paper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1wBeiXTPIr"
      },
      "source": [
        "### DP PCA layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjwa5LgQTPIs"
      },
      "outputs": [],
      "source": [
        "class DP_PCA(tf.keras.layers.Layer):\n",
        "    def __init__(self, pca_components, seed, std, covar):\n",
        "        super(DP_PCA, self).__init__()\n",
        "        self.pca_components = pca_components\n",
        "        self.seed = seed\n",
        "        self.std = std\n",
        "        self.covar = covar\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        a = 1 # placeholder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #inputs = tf.linalg.normalize(inputs, ord=2, axis=-1)\n",
        "        #inputs = tf.math.l2_normalize(inputs, axis=-1)\n",
        "        #covar = tf.einsum('ji,jk->ik', inputs, inputs)\n",
        "        #print(tf.shape(covar))\n",
        "        # add noise\n",
        "        covar = self.covar # copy\n",
        "        #shape = tf.shape(covar)\n",
        "        #noise = tf.random.normal(shape, mean = 0, stddev = self.std, dtype=tf.dtypes.float32, seed=self.seed)\n",
        "        #noise = np.tril(noise) + np.tril(noise, -1).T\n",
        "        #noise = noise + noise.T - tf.linalg.diag_part(noise)\n",
        "        #noise = (noise + noise.T)/2\n",
        "        #print(noise == noise.T)\n",
        "        #covar += noise\n",
        "        # add vectors\n",
        "        e_values, e_vectors = tf.linalg.eigh(covar)\n",
        "        return tf.einsum('ij,jk->ik', inputs, e_vectors[:,-self.pca_components:])\n",
        "        #return tf.einsum('ji,ik->jk', inputs, e_vectors[:,-self.pca_components:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pEm3ww8TPIs"
      },
      "source": [
        "### Base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU3qcM9WBcMh"
      },
      "outputs": [],
      "source": [
        "def base_model():\n",
        "    inputs = tf.keras.Input(shape=(784,), name='digits')\n",
        "    x = DP_PCA(60, seed, std_pca, covar)(inputs)\n",
        "    x = tf.keras.layers.Dense(1000, activation='relu', name='dense_1')(x)\n",
        "    outputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxaHy1NYPGSb"
      },
      "source": [
        "## Define Optimizer and Loss Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5B3vh6fs84i"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate = lr_sgd) # [0.01,0.07] stable, best at 0.05\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.compat.v1.losses.Reduction.NONE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1fJsdYIPTb8"
      },
      "source": [
        "## Define Metrics & Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pa_x-5-CH_V"
      },
      "outputs": [],
      "source": [
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0zABOHYTPIt"
      },
      "source": [
        "## Moment accountant class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5jn-hjhTPIu"
      },
      "outputs": [],
      "source": [
        "class moment_accountant():\n",
        "    def __init__(self,seed, params: dict,deltaFixed = False, epsFixed= False, debug = False, **kwargs):\n",
        "        \n",
        "        # moment accountant hyperparameters\n",
        "        self.mixN = 1000 # samples for mixture of moment arrays\n",
        "        self.debug = debug\n",
        "        self.seed = seed\n",
        "        \n",
        "        # constants \n",
        "        self.maxOrder = params[\"maxOrder\"] # maximum moment order \n",
        "        self.lambd = np.arange(0,self.maxOrder+1)\n",
        "        #self.lambd = np.array([0])\n",
        "        self.lambdaN = len(self.lambd) # number of lambdas to check\n",
        "        print(f\"Maxorder = {self.maxOrder}, with order array:\")\n",
        "        print(self.lambd)\n",
        "        self.sigma = params[\"sigma\"]\n",
        "        self.q = params[\"q\"]\n",
        "        self.T = params[\"T\"]\n",
        "        \n",
        "        # booleans \n",
        "        self.deltaFixed = deltaFixed\n",
        "        self.epsFixed = epsFixed\n",
        "        \n",
        "        if self.deltaFixed:\n",
        "            if self.epsFixed:\n",
        "                raise Exception(\"Choose ONLY epsilon or delta as fixed\")\n",
        "            # in case delta is held fixed\n",
        "            print(\"keeping delta fixed\")\n",
        "            self.delta = kwargs[\"delta\"]\n",
        "            self.th_epsilon = kwargs[\"th_epsilon\"]\n",
        "        elif self.epsFixed:\n",
        "            # in case epsilon is held fixed\n",
        "            print(\"keeping epsilon fixed\")\n",
        "            self.epsilon = kwargs[\"epsilon\"]\n",
        "            self.th_delta = kwargs[\"th_delta\"]\n",
        "        else:\n",
        "            raise Exception(\"Choose EITHER epsilon or delta as fixed\")\n",
        "        \n",
        "        #self.e1_mu0, self.e1_mu, self.e2_mu0, self.e2_mu = self._setup_mixNormNP() # obtain random sample arrays NUMPY\n",
        "        self.e1_mu0, self.e1_mu, self.e2_mu0, self.e2_mu = self._setup_mixNormTF() # obtain random sample arrays TENSORFLOW\n",
        "        self.alpha = self._compute_moment(self.lambd)\n",
        "        \n",
        "        # initializations \n",
        "        self.alphaSum = 0 # moment\n",
        "        self.lambdArgmin = []\n",
        "        self.iterations = 0\n",
        "        self.deltaList = []\n",
        "        self.epsList = []\n",
        "        # =================================\n",
        "        print(\"moment accountant setup complete\")\n",
        "        \n",
        "    def _setup_mixNormTF(self):\n",
        "        '''\n",
        "        Mixture of gaussians by tensorflow, so no assumptions used\n",
        "        https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Mixture\n",
        "        '''\n",
        "        \n",
        "        '''\n",
        "        CHECKED - WORKING CORRECTLY\n",
        "        '''\n",
        "        # setup normal dis & mixture of normals arrays\n",
        "        np.random.seed(self.seed) # set seed\n",
        "        mu_0 = np.random.normal(0, self.sigma, (self.mixN))\n",
        "        # analytical mean & var for mix\n",
        "        \n",
        "        # setup gaussian mixture\n",
        "        dismix_mu = tfp.distributions.Mixture(cat=tfp.distributions.Categorical(probs=[1.-self.q, self.q]),\n",
        "                                    components=[tfp.distributions.Normal(loc=0., scale=self.sigma),\n",
        "                                                tfp.distributions.Normal(loc=1., scale=self.sigma)])\n",
        "        mu = dismix_mu.sample(sample_shape=(self.mixN), seed=self.seed).numpy()\n",
        "\n",
        "        # find pdf values for z's \n",
        "        e1_mu0 = sc.norm.pdf(mu_0,loc = 0, scale = self.sigma)\n",
        "        e1_mu = dismix_mu.prob(mu_0).numpy()\n",
        "        \n",
        "        e2_mu0 = sc.norm.pdf(mu,loc = 0, scale = self.sigma)\n",
        "        e2_mu = dismix_mu.prob(mu).numpy()\n",
        "\n",
        "        return e1_mu0, e1_mu, e2_mu0, e2_mu\n",
        "                            \n",
        "    def _compute_moment(self, lambd: np.array):\n",
        "        '''\n",
        "        CHECKED - WORKING CORRECTLY\n",
        "        '''\n",
        "        # computes unbiased expectation for E1 & E2, then the moment alpha \n",
        "        lambd = np.broadcast_to(np.expand_dims(lambd, -1), (self.lambdaN, self.mixN)) # broadcast\n",
        "        #E1 = 1/self.mixN*np.sum(np.transpose(np.power(np.transpose(self.mu_0/self.mu),lambd)), axis = 0)\n",
        "        #E2 = 1/self.mixN*np.sum(np.transpose(np.power(np.transpose(self.mu/self.mu_0),lambd)), axis = 0)\n",
        "        E1 = np.nanmean(np.transpose(np.power(np.transpose(self.e1_mu0/self.e1_mu),lambd)), axis = 0)\n",
        "        '''\n",
        "        note that due to setup E1 will always be < 1 since denom > num always\n",
        "        '''\n",
        "        E2 = np.nanmean(np.transpose(np.power(np.transpose(self.e2_mu/self.e2_mu0),lambd)), axis = 0)\n",
        "        '''\n",
        "        note that due to setup E2 will always be > 1 since denom < num always\n",
        "        '''\n",
        "        #alpha = np.log(np.maximum(E1,E2))\n",
        "        alpha = np.log(np.maximum(E1,E2))\n",
        "        return alpha\n",
        "    \n",
        "    def compute_deltaEps(self):\n",
        "        # tail bound\n",
        "        #alpha = self._compute_moment(self.lambd) + self.alpha # note that this is the log moment!\n",
        "        alpha = self.alphaSum + self.alpha # note that this is the log moment!\n",
        "        self.alphaSum = alpha # update moment\n",
        "        if self.epsFixed:\n",
        "            # epsilon is kept fixed, compute delta\n",
        "            epsilon = self.epsilon\n",
        "            delta = np.min(np.exp(alpha-self.lambd*epsilon))\n",
        "            # TODO remove inf or nan <- does not seem necessary\n",
        "            if self.debug:\n",
        "                ind = np.argmin(np.exp(alpha-self.lambd*epsilon))\n",
        "                self.lambdArgmin.append(self.lambd[ind])\n",
        "        if self.deltaFixed:\n",
        "            # delta is kept fixed, compute epsilon\n",
        "            delta = self.delta\n",
        "            epsilon = (alpha-np.log(delta))/self.lambd\n",
        "            if self.debug:\n",
        "                ind = np.argmin(epsilon)\n",
        "                self.lambdArgmin.append(self.lambd[ind])\n",
        "            epsilon = np.min(epsilon)\n",
        "            # TODO remove inf or nan <- does not seem necessary\n",
        "        self.epsList.append(epsilon)\n",
        "        self.deltaList.append(delta)\n",
        "        return delta, epsilon\n",
        "    \n",
        "    def check_thresholds(self, delta: float, epsilon: float):\n",
        "        go = True\n",
        "        if self.epsFixed:\n",
        "            if self.th_delta < delta: \n",
        "                # delta threshold exceeded\n",
        "                go = False\n",
        "        if self.deltaFixed:\n",
        "            if self.th_epsilon < epsilon:\n",
        "                # epsilon threshold exceeded\n",
        "                go = False\n",
        "        return go\n",
        "    \n",
        "    def plot_traces(self):\n",
        "        if len(self.epsList) == 0:\n",
        "            raise Exception(\"Apply iterations on accountant instance before calling this function\")\n",
        "        elif not self.debug:\n",
        "            raise Exception(\"Debug was set to false, thus not all relevant data was collected\")\n",
        "        else:\n",
        "            # gather data\n",
        "            epsilon = np.array(self.epsList)\n",
        "            delta = np.array(self.deltaList)\n",
        "            lambdas = np.array(self.lambdArgmin)\n",
        "            print(f\"Delta fixed = {self.deltaFixed}| Last delta = {delta[-1]}\")\n",
        "            print(f\"Epsilon fixed = {self.epsFixed}| Last epsilon = {epsilon[-1]}\")\n",
        "            print(\"Fixed parameter will not be plotted \\n NOTE: Iteration arrays are returned\")\n",
        "            # plotting\n",
        "            iterations = np.arange(0,len(epsilon))\n",
        "            plt.figure()\n",
        "            if self.deltaFixed:\n",
        "                plt.plot(iterations,epsilon, label = 'epsilon')\n",
        "            else:\n",
        "                plt.plot(iterations,delta, label = 'delta')\n",
        "            #plt.legend(loc='upper left')\n",
        "            plt.title(\"epsilon or delta over iterations\")\n",
        "            \n",
        "            plt.figure()\n",
        "            plt.plot(iterations,lambdas)\n",
        "            plt.title(\"Lambda chosen over iterations\")\n",
        "            \n",
        "        return delta, epsilon, lambdas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVFI54MpQUDp"
      },
      "source": [
        "## Differential privary variant of SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7s_KTqfTPI2"
      },
      "outputs": [],
      "source": [
        "class DP_SGD:\n",
        "    def __init__(self, lr = 0.01, sigma = 0.2, gs = 10, C = 1, seed = 2): # TODO; check defaults\n",
        "        self.lr = lr # learning rate\n",
        "        self.sigma = sigma # sigma, noise scale\n",
        "        self.gs = gs # group size\n",
        "        self.C = C # gradient norm bound\n",
        "        self.seed = seed\n",
        "        self.std = sigma*C # for noise addition\n",
        "        #self.loss_object = ? # define this here\n",
        "        \n",
        "    def apply_gradients(self, optimizer, model, loss_object, x, y):\n",
        "\n",
        "        #n = tf.shape(x).numpy()\n",
        "        ## run loop\n",
        "        with tf.GradientTape(persistent = True) as tape:\n",
        "            y_pred = model(x)\n",
        "            loss = loss_object(y, y_pred)\n",
        "            loss_red = tf.reduce_sum(loss, axis = 0)\n",
        "            \n",
        "            \n",
        "        #########\n",
        "        # MAIN PROBLEM: I only get a single set of gradients even if i have a loss function which outputs loss for every sample\n",
        "        # do you want me to parallelize this myself\n",
        "        ########\n",
        "        ## obtain gradients    \n",
        "        #grad = tape.batch_jacobian(loss, model.trainable_weights)\n",
        "        grad = tape.jacobian(loss, model.trainable_weights, parallel_iterations = None, experimental_use_pfor= False)\n",
        "        \n",
        "        ## clip gradients per layer\n",
        "        for l in range(len(grad)):\n",
        "            #clipper = tf.norm(grad[l], ord = 2, axis = 0)\n",
        "            dims = len(tf.shape(grad[l]))\n",
        "            clipper = tf.math.square(grad[l])\n",
        "            \n",
        "            if dims > 2:\n",
        "                # kernel layers\n",
        "                clipper = tf.reduce_sum(clipper, axis = [1,2])\n",
        "            else:\n",
        "                # bias layer\n",
        "                clipper = tf.reduce_sum(clipper, axis = -1)\n",
        "                \n",
        "            clipper = tf.math.sqrt(clipper)\n",
        "            clipper = tf.math.maximum(tf.constant([1], dtype = tf.dtypes.float32),clipper/self.C)\n",
        "            if dims > 2:\n",
        "                # kernel layers\n",
        "                clipper = tf.broadcast_to(tf.expand_dims(tf.expand_dims(clipper, -1), -1),tf.shape(grad[l]))\n",
        "            else:\n",
        "                # bias layer\n",
        "                clipper = tf.broadcast_to(tf.expand_dims(clipper, -1),tf.shape(grad[l]))\n",
        "            grad[l] = tf.math.divide(grad[l],clipper) # override\n",
        "                \n",
        "        ## add noise\n",
        "        for l in range(len(grad)): # loop over layers\n",
        "            grad_red = tf.math.reduce_sum(grad[l], axis = 0)\n",
        "            shape = tf.shape(grad_red)\n",
        "            noise = tf.random.normal(shape, mean = 0, stddev = self.std, dtype=tf.dtypes.float32, seed=self.seed)\n",
        "            grad[l] = tf.add(grad_red, noise)/self.gs\n",
        "            \n",
        "\n",
        "\n",
        "        ## descent\n",
        "        #print(grad)\n",
        "        '''\n",
        "        print(f'HELOO={len(grad)}')\n",
        "        print(f'HELOO={len(model.trainable_weights)}')\n",
        "        ga = []\n",
        "        ma = []\n",
        "        print(type(model.trainable_weights))\n",
        "        for l in range(len(model.trainable_weights)):\n",
        "            ga.append(tf.shape(grad[l]).numpy())\n",
        "            ma.append(tf.shape(model.trainable_weights[l]).numpy())\n",
        "        zipp = list(zip(ga,ma))\n",
        "        print(zipp)\n",
        "        \n",
        "        #print(model.trainable_weights)\n",
        "        '''\n",
        "        \n",
        "        optimizer.apply_gradients(zip(grad,model.trainable_weights))\n",
        "            \n",
        "        ## collect\n",
        "    \n",
        "        return y_pred,loss_red \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHGlbbJHTPI3"
      },
      "source": [
        "## Building Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BblnrmH0TPI3"
      },
      "outputs": [],
      "source": [
        "def apply_gradient(optimizer, model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x)\n",
        "        loss_value = loss_object(y_true=y, y_pred=logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights)) # ZIP ENSURES THAT GRADIENTS IS APPLIED TO EVERY LAYER CORRECTLY (SINCE EVERY LAYER HAS W & b != not 1 variable)\n",
        "\n",
        "    return logits, loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4aoNUeoTPI4"
      },
      "outputs": [],
      "source": [
        "def train_data_for_one_epoch(dp_sgd, optimizer, model, loss_object, moment_accountant):\n",
        "    losses = []\n",
        "    pbar = tqdm(total=len(list(enumerate(train))), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ') # progress bar\n",
        "    go = True\n",
        "    step = 0 \n",
        "    for x_batch_train, y_batch_train in train:\n",
        "        step += 1\n",
        "        logits, loss_value = dp_sgd.apply_gradients(optimizer, model, loss_object, x_batch_train, y_batch_train)\n",
        "        #logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)\n",
        "        delta, eps = moment_accountant.compute_deltaEps()\n",
        "        \n",
        "        losses.append(loss_value)\n",
        "\n",
        "        train_acc_metric(y_batch_train, logits)\n",
        "        pbar.set_description(\"Training loss for step %s: %.4f\" % (int(step), float(loss_value)))\n",
        "        pbar.update()\n",
        "        \n",
        "        if not moment_accountant.check_thresholds(delta, epsilon):\n",
        "            go = False\n",
        "            break\n",
        "    print(f\"Delta = {delta} | Epsilon = {epsilon}\")\n",
        "    return losses, go "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBZyXnuUQxVn"
      },
      "source": [
        "At the end of each epoch you have to validate the model on the test dataset. The following function calculates the loss on test dataset and updates the states of the validation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gLJyAJE0YRc"
      },
      "outputs": [],
      "source": [
        "def perform_validation():\n",
        "    losses = []\n",
        "    for x_val, y_val in test:\n",
        "        val_logits = model(x_val)\n",
        "        val_loss = loss_object(y_val, val_logits)\n",
        "        #val_loss = tf.reduce_sum(val_loss, axis = 0).numpy()\n",
        "        losses.append(val_loss)\n",
        "        val_acc_metric(y_val, val_logits)\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOO1x3VyuPUV"
      },
      "outputs": [],
      "source": [
        "## INITIALIZE \n",
        "model = base_model()\n",
        "model.layers[2].trainable = False # pca layer\n",
        "DPSGD = DP_SGD(lr_sgd,std_sgd, gs, C, seed)\n",
        "if epsFixed: \n",
        "    print(\"\\n Epsilon kept fixed \\n\")\n",
        "    accountant = moment_accountant(seed, parameters_ma, deltaFixed = deltaFixed, epsFixed= epsFixed, debug = debug, epsilon = epsilon, th_delta = th_delta)\n",
        "else:\n",
        "    print(\"\\n Delta kept fixed \\n\")\n",
        "    #delta fixed\n",
        "    accountant = moment_accountant(seed, parameters_ma, deltaFixed = deltaFixed, epsFixed= epsFixed, debug = debug, delta = delta, th_epsilon = th_epsilon)\n",
        "\n",
        "# Iterate over epochs.\n",
        "epochs = 125 #18\n",
        "epochs_val_losses, epochs_train_losses = [], []\n",
        "val_acc_list, train_acc_list = [], []\n",
        "for epoch in range(epochs):\n",
        "    print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "    losses_train, go = train_data_for_one_epoch(DPSGD, optimizer, model, loss_object, accountant)\n",
        "    train_acc = train_acc_metric.result()\n",
        "\n",
        "    losses_val = perform_validation()\n",
        "    val_acc = val_acc_metric.result()\n",
        "\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    losses_train_mean = np.mean(losses_train)\n",
        "    losses_val_mean = np.mean(losses_val)\n",
        "    epochs_val_losses.append(losses_val_mean)\n",
        "    epochs_train_losses.append(losses_train_mean)\n",
        "\n",
        "    print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc)))\n",
        "\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()\n",
        "    if not go:\n",
        "        print(f\"\\n Stopping due to privacy loss at epoch {epoch}/{epochs} \\n\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltKpkpzKK_Up"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'accuracies', 'wb') as f:\n",
        "    np.save(f, np.array(val_acc_list))\n",
        "    np.save(f, np.array(train_acc_list))\n",
        "#np.load(accuracies)\n",
        "#val_acc_list, train_acc_list"
      ],
      "metadata": {
        "id": "i9j1UahXAhXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfGc-gMPLCDn"
      },
      "source": [
        "### Plots for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(train_acc,val_acc,deltas):\n",
        "\n",
        "  fig, ax1 = plt.subplots()\n",
        "  x = np.arange(1,len(train_acc)+1)\n",
        "  ax1.plot(x,train_acc,'bs',label='training accuracy')\n",
        "  ax1.plot(x,val_acc,'r',label='validation accuracy')\n",
        "  ax1.set_ylim([0.7,1])\n",
        "  ax1.set_xticks(np.arange(0, max(x)+1, 10.0))\n",
        "  ax1.set_xlabel('Epochs' )\n",
        "  ax1.set_ylabel('accuracy',color='r')\n",
        "  ax1.legend()\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.yaxis.tick_right()\n",
        "  ax2.yaxis.set_label_position('right')\n",
        "  ax2.xaxis.tick_top()\n",
        "  ax2.xaxis.set_label_position('top')\n",
        "\n",
        "  ax2.plot(x,deltas, marker = 'x', color = 'black')\n",
        "  #ax2.set_ylim([0,1])\n",
        "  ax2.set_xlabel('epsilon')\n",
        "  ax2.set_ylabel('deltas')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plot_graph(train_acc_list,val_acc_list,delta)"
      ],
      "metadata": {
        "id": "AvvSnNdPY3gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjzIlGipJwC_"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_metric, val_metric, metric_name, title, ylim=5):\n",
        "    plt.title(title)\n",
        "    #plt.ylim(0,ylim)\n",
        "    plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "    x = np.arange(1,len(train_metric)+1)\n",
        "    plt.plot(x,train_metric,color='blue',label=metric_name)\n",
        "    plt.plot(x,val_metric,color='green',label='val_' + metric_name)\n",
        "    plt.legend()\n",
        "\n",
        "plot_metrics(epochs_train_losses, epochs_val_losses, \"Loss\", \"Loss\", ylim=10.0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DP_MNIST.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Tensor",
      "language": "python",
      "name": "tensor"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}